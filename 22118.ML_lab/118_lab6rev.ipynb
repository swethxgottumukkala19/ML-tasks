{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ab8454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Initialize Gaussian Naive Bayes classifier\n",
    "classifier = GaussianNB()\n",
    "\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c74cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the necessary preprocessing techniques on IRIS dataset.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features (optional)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93b7379",
   "metadata": {},
   "source": [
    "In addition to Gaussian Naive Bayes, which assumes that features follow a normal (Gaussian) distribution, there are other variants of Naive Bayes classifiers that are commonly used for different types of data distributions. Some of the other variants of Naive Bayes classifiers include:\n",
    "\n",
    "Multinomial Naive Bayes:\n",
    "\n",
    "Suitable for text classification tasks where features represent word counts or frequency of occurrence (e.g., bag-of-words model).\n",
    "Assumes that features are generated from a multinomial distribution.\n",
    "Often used in natural language processing (NLP) tasks such as document classification and spam filtering.\n",
    "\n",
    "Bernoulli Naive Bayes:\n",
    "\n",
    "Similar to Multinomial Naive Bayes but specifically designed for binary feature vectors (i.e., presence or absence of a feature).\n",
    "Assumes that features are binary (e.g., presence or absence of words in a document).\n",
    "Also commonly used in text classification tasks, particularly when using binary feature representations.\n",
    "\n",
    "Complement Naive Bayes:\n",
    "\n",
    "Variant of Multinomial Naive Bayes that is designed to address class imbalance problems in text classification.\n",
    "Computes statistics complementary to those of the original class to adjust for imbalanced class distributions.\n",
    "Particularly effective for text classification tasks with highly imbalanced class distributions.\n",
    "\n",
    "Categorical Naive Bayes:\n",
    "\n",
    "Extension of Multinomial Naive Bayes that is suitable for categorical features with a fixed number of categories.\n",
    "Assumes that features are drawn from a categorical distribution.\n",
    "Useful for classification tasks involving categorical data, such as customer segmentation or product recommendation.\n",
    "\n",
    "These variants of Naive Bayes classifiers offer different assumptions about the underlying data distributions and are suitable for different types of data. It's essential to choose the appropriate variant based on the characteristics of the dataset and the nature of the classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87eefa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify other variants of Naive Bayes classifier.\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Initialize Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "# replace Multinomial with Complement, Bernoulli, Categorical for others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3b08002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: Precision=1.0, Recall=1.0, F1-score=1.0, Support=10\n",
      "Class 1: Precision=1.0, Recall=1.0, F1-score=1.0, Support=9\n",
      "Class 2: Precision=1.0, Recall=1.0, F1-score=1.0, Support=11\n"
     ]
    }
   ],
   "source": [
    "# Compute other metric values using confusion matrix.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "# Assuming y_true and y_pred contain the true and predicted labels, respectively\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Compute precision, recall, F1-score, and support\n",
    "precision, recall, f1_score, support = precision_recall_fscore_support(y_test, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "for i in range(len(precision)):\n",
    "    print(f\"Class {i}: Precision={precision[i]}, Recall={recall[i]}, F1-score={f1_score[i]}, Support={support[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9712c323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Apply feature engineering and then apply Naive Bayes classifier and print its performance measures.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform feature engineering (if needed)\n",
    "# For example, you can apply scaling, normalization, or create new features here\n",
    "\n",
    "# Initialize Gaussian Naive Bayes classifier\n",
    "classifier = GaussianNB()\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cd90b1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multiclass format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m y_pred_proba \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Compute ROC curve\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m fpr, tpr, thresholds \u001b[38;5;241m=\u001b[39m roc_curve(y_test, y_pred_proba)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Compute AUC\u001b[39;00m\n\u001b[0;32m     18\u001b[0m roc_auc \u001b[38;5;241m=\u001b[39m auc(fpr, tpr)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1094\u001b[0m, in \u001b[0;36mroc_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    993\u001b[0m     {\n\u001b[0;32m    994\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1003\u001b[0m     y_true, y_score, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, drop_intermediate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m ):\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m \n\u001b[0;32m   1007\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;124;03m    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1094\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m _binary_clf_curve(\n\u001b[0;32m   1095\u001b[0m         y_true, y_score, pos_label\u001b[38;5;241m=\u001b[39mpos_label, sample_weight\u001b[38;5;241m=\u001b[39msample_weight\n\u001b[0;32m   1096\u001b[0m     )\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m     \u001b[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m     \u001b[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:803\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    801\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[1;32m--> 803\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[0;32m    805\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m    806\u001b[0m y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n",
      "\u001b[1;31mValueError\u001b[0m: multiclass format is not supported"
     ]
    }
   ],
   "source": [
    "# Print AUC-ROC curve.\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_test and y_pred_proba are defined\n",
    "# y_test: True labels of the test set\n",
    "# y_pred_proba: Predicted probabilities for the positive class\n",
    "\n",
    "# Assuming classifier is your trained classifier or model\n",
    "# Predict probabilities for the positive class\n",
    "y_pred_proba = classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Compute AUC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7e1528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the covariance matrix of iris dataset and use it in classification.\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Compute covariance matrix\n",
    "cov_matrix = np.cov(X.T)  # Transpose X to compute covariance matrix of features\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Gaussian Naive Bayes classifier\n",
    "classifier = GaussianNB()\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy without using covariance matrix:\", accuracy)\n",
    "\n",
    "# Update classifier with covariance matrix\n",
    "classifier.sigma_ = cov_matrix  # Update covariance matrix in the classifier\n",
    "\n",
    "# Predictions on the test set with updated classifier\n",
    "y_pred_cov = classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy with updated classifier\n",
    "accuracy_cov = accuracy_score(y_test, y_pred_cov)\n",
    "print(\"Accuracy with covariance matrix:\", accuracy_cov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b315c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the statistics of every feature of iris dataset.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Create a pandas DataFrame from the feature matrix\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "\n",
    "# Print statistics of every feature\n",
    "print(\"Statistics of every feature:\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11c0b85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: GaussianNB\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multiclass format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred))\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# AUC-ROC curve\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m fpr, tpr, thresholds \u001b[38;5;241m=\u001b[39m roc_curve(y_test, y_pred)\n\u001b[0;32m     47\u001b[0m roc_auc \u001b[38;5;241m=\u001b[39m auc(fpr, tpr)\n\u001b[0;32m     48\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1094\u001b[0m, in \u001b[0;36mroc_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    993\u001b[0m     {\n\u001b[0;32m    994\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1003\u001b[0m     y_true, y_score, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, drop_intermediate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m ):\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m \n\u001b[0;32m   1007\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;124;03m    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1094\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m _binary_clf_curve(\n\u001b[0;32m   1095\u001b[0m         y_true, y_score, pos_label\u001b[38;5;241m=\u001b[39mpos_label, sample_weight\u001b[38;5;241m=\u001b[39msample_weight\n\u001b[0;32m   1096\u001b[0m     )\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m     \u001b[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m     \u001b[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:803\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    801\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[1;32m--> 803\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[0;32m    805\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m    806\u001b[0m y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n",
      "\u001b[1;31mValueError\u001b[0m: multiclass format is not supported"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing techniques: Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Variant of Naive Bayes classifiers\n",
    "classifiers = {\n",
    "    'GaussianNB': GaussianNB(),\n",
    "#     'MultinomialNB': MultinomialNB(),\n",
    "#     'ComplementNB': ComplementNB(),\n",
    "#     'BernoulliNB': BernoulliNB()\n",
    "}\n",
    "\n",
    "# Train and evaluate Naive Bayes classifiers\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    \n",
    "    # Compute confusion matrix and other metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"Classifier: {name}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # AUC-ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic - ' + name)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# Feature engineering: Computing covariance matrix\n",
    "cov_matrix = np.cov(X_train_scaled.T)\n",
    "print(\"Covariance Matrix:\")\n",
    "print(cov_matrix)\n",
    "\n",
    "# Print statistics of every feature\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "print(\"Statistics of every feature:\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "528b3f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1-score: 1.0\n",
      "Covariance Matrix:\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  6.78654762e-01 -3.95616246e-02  1.24462745e+00\n",
      "   4.96708683e-01  8.06219867e+00  1.88069622e+00  9.91041688e+00\n",
      "   3.72001324e+00 -2.94060644e-01  3.83677444e+00  1.55539426e+00\n",
      "   9.09170182e+00  3.20814223e+00  1.13214580e+00]\n",
      " [ 0.00000000e+00 -3.95616246e-02  2.01711485e-01 -3.40061625e-01\n",
      "  -1.24845938e-01 -4.26938515e-01  9.98839496e-01 -1.94933459e+00\n",
      "  -6.90613025e-01  1.26340868e+00 -4.51765686e-01 -2.12586835e-01\n",
      "  -1.90022213e+00 -5.89899580e-01 -1.76429412e-01]\n",
      " [ 0.00000000e+00  1.24462745e+00 -3.40061625e-01  3.07071148e+00\n",
      "   1.26893557e+00  1.46115451e+01  2.00873277e+00  2.22916650e+01\n",
      "   8.72853277e+00 -2.24463249e+00  8.76856919e+00  3.76041737e+00\n",
      "   2.14109737e+01  7.74989580e+00  2.80257311e+00]\n",
      " [ 0.00000000e+00  4.96708683e-01 -1.24845938e-01  1.26893557e+00\n",
      "   5.65938375e-01  5.80240196e+00  8.76957983e-01  9.10679132e+00\n",
      "   3.83200420e+00 -8.33515406e-01  3.69756443e+00  1.70401961e+00\n",
      "   8.84512045e+00  3.41006303e+00  1.30348739e+00]\n",
      " [ 0.00000000e+00  8.06219867e+00 -4.26938515e-01  1.46115451e+01\n",
      "   5.80240196e+00  9.63642963e+01  2.26174005e+01  1.17651687e+02\n",
      "   4.39589695e+01 -3.26028766e+00  4.53221990e+01  1.82674995e+01\n",
      "   1.07958605e+02  3.79710519e+01  1.33594236e+01]\n",
      " [ 0.00000000e+00  1.88069622e+00  9.98839496e-01  2.00873277e+00\n",
      "   8.76957983e-01  2.26174005e+01  1.14497927e+01  2.01294806e+01\n",
      "   7.88623559e+00  6.12348076e+00  9.63025366e+00  3.78313235e+00\n",
      "   1.79224515e+01  6.84982660e+00  2.61922294e+00]\n",
      " [ 0.00000000e+00  9.91041688e+00 -1.94933459e+00  2.22916650e+01\n",
      "   9.10679132e+00  1.17651687e+02  2.01294806e+01  1.67650252e+02\n",
      "   6.47178462e+01 -1.31387136e+01  6.54480936e+01  2.75421387e+01\n",
      "   1.59497654e+02  5.72669693e+01  2.05646021e+01]\n",
      " [ 0.00000000e+00  3.72001324e+00 -6.90613025e-01  8.72853277e+00\n",
      "   3.83200420e+00  4.39589695e+01  7.88623559e+00  6.47178462e+01\n",
      "   2.67154036e+01 -4.71778408e+00  2.60323261e+01  1.17273981e+01\n",
      "   6.22733205e+01  2.37194800e+01  8.98230996e+00]\n",
      " [ 0.00000000e+00 -2.94060644e-01  1.26340868e+00 -2.24463249e+00\n",
      "  -8.33515406e-01 -3.26028766e+00  6.12348076e+00 -1.31387136e+01\n",
      "  -4.71778408e+00  7.99613812e+00 -3.27139318e+00 -1.52286232e+00\n",
      "  -1.27958517e+01 -4.05007391e+00 -1.24513185e+00]\n",
      " [ 0.00000000e+00  3.83677444e+00 -4.51765686e-01  8.76856919e+00\n",
      "   3.69756443e+00  4.53221990e+01  9.63025366e+00  6.54480936e+01\n",
      "   2.60323261e+01 -3.27139318e+00  2.70494059e+01  1.16024299e+01\n",
      "   6.27784354e+01  2.31863180e+01  8.55349441e+00]\n",
      " [ 0.00000000e+00  1.55539426e+00 -2.12586835e-01  3.76041737e+00\n",
      "   1.70401961e+00  1.82674995e+01  3.78313235e+00  2.75421387e+01\n",
      "   1.17273981e+01 -1.52286232e+00  1.16024299e+01  5.34469006e+00\n",
      "   2.67260136e+01  1.04540532e+01  4.04813067e+00]\n",
      " [ 0.00000000e+00  9.09170182e+00 -1.90022213e+00  2.14109737e+01\n",
      "   8.84512045e+00  1.07958605e+02  1.79224515e+01  1.59497654e+02\n",
      "   6.22733205e+01 -1.27958517e+01  6.27784354e+01  2.67260136e+01\n",
      "   1.54200876e+02  5.58799468e+01  2.02667305e+01]\n",
      " [ 0.00000000e+00  3.20814223e+00 -5.89899580e-01  7.74989580e+00\n",
      "   3.41006303e+00  3.79710519e+01  6.84982660e+00  5.72669693e+01\n",
      "   2.37194800e+01 -4.05007391e+00  2.31863180e+01  1.04540532e+01\n",
      "   5.58799468e+01  2.13611994e+01  8.12992450e+00]\n",
      " [ 0.00000000e+00  1.13214580e+00 -1.76429412e-01  2.80257311e+00\n",
      "   1.30348739e+00  1.33594236e+01  2.61922294e+00  2.05646021e+01\n",
      "   8.98230996e+00 -1.24513185e+00  8.55349441e+00  4.04813067e+00\n",
      "   2.02667305e+01  8.12992450e+00  3.22355571e+00]]\n",
      "Statistics for sepal length (cm):\n",
      "  - Mean: 5.843333333333334\n",
      "  - Standard Deviation: 0.8253012917851409\n",
      "  - Minimum: 4.3\n",
      "  - Maximum: 7.9\n",
      "Statistics for sepal width (cm):\n",
      "  - Mean: 3.0573333333333337\n",
      "  - Standard Deviation: 0.4344109677354946\n",
      "  - Minimum: 2.0\n",
      "  - Maximum: 4.4\n",
      "Statistics for petal length (cm):\n",
      "  - Mean: 3.7580000000000005\n",
      "  - Standard Deviation: 1.759404065775303\n",
      "  - Minimum: 1.0\n",
      "  - Maximum: 6.9\n",
      "Statistics for petal width (cm):\n",
      "  - Mean: 1.1993333333333336\n",
      "  - Standard Deviation: 0.7596926279021594\n",
      "  - Minimum: 0.1\n",
      "  - Maximum: 2.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Feature engineering - example: adding polynomial features\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Gaussian Naive Bayes classifier\n",
    "classifier = GaussianNB()\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Precision, Recall, F1-score\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Compute covariance matrix\n",
    "cov_matrix = np.cov(X_train.T)\n",
    "print(\"Covariance Matrix:\\n\", cov_matrix)\n",
    "\n",
    "# Print statistics of every feature\n",
    "feature_names = iris.feature_names\n",
    "for i, feature_name in enumerate(feature_names):\n",
    "    feature_data = X[:, i]\n",
    "    print(f\"Statistics for {feature_name}:\")\n",
    "    print(f\"  - Mean: {np.mean(feature_data)}\")\n",
    "    print(f\"  - Standard Deviation: {np.std(feature_data)}\")\n",
    "    print(f\"  - Minimum: {np.min(feature_data)}\")\n",
    "    print(f\"  - Maximum: {np.max(feature_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454f2dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
